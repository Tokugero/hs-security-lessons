# Install prerequisites on all hosts
#############################################
- name: Add apt_key
  block:
    - name: K8S Repo
      get_url:
        url: https://pkgs.k8s.io/core:/stable:/v{{ k8s_version }}/deb/Release.key
        dest: /etc/apt/keyrings/kubernetes-apt-keyring.asc

    - name: Add repository
      apt_repository:
        filename: kubernetes.list
        repo: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.asc] https://pkgs.k8s.io/core:/stable:/v{{ k8s_version }}/deb/ /
        state: present

- name: Install/Update k8s packages
  apt:
    pkg:
      - conntrack
      - ethtool
      - iproute2
      - iptables
      - kubelet={{ k8s_version }}.1-1.1
      - kubeadm={{ k8s_version }}.1-1.1
      - kubectl={{ k8s_version }}.1-1.1
      - python3-kubernetes
    state: present

- name: Hold kubelet
  dpkg_selections:
    name: kubelet
    selection: hold

- name: Hold kubeadm
  dpkg_selections:
    name: kubeadm
    selection: hold

- name: Hold kubectl
  dpkg_selections:
    name: kubectl
    selection: hold

- name: Disable swap
  command: /usr/sbin/swapoff -a

- name: Disable systemd swap
  command: systemctl mask "dev-*.swap"

- name: Update fstab
  lineinfile:
    dest: /etc/fstab
    regexp: "^(UUID=.*swap.*)"
    line: '# \1'
    backrefs: yes
    state: present

- name: Update config.toml to allow for insecure local registry
  lineinfile:
    dest: /etc/containerd/config.toml
    regexp: 'config_path = ""'
    line: '      config_path = "/etc/containerd/certs.d"'

- name: Docker pull registry
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Pull registry
      docker_image:
        name: registry:2
        source: pull

    - name: Create directory for registry
      file:
        path: /mnt/usbstorage/registry
        state: directory

    - name: Copy docker-compsoe
      copy:
        src: ptc-registry-docker-compose.yml
        dest: /mnt/usbstorage/registry/docker-compose.yml

    - name: Copy config override
      copy:
        src: ptc-registry-config.yml
        dest: /mnt/usbstorage/registry/config.yml

    - name: Start registry
      shell: docker compose -f /mnt/usbstorage/registry/docker-compose.yml up -d

- name: Add host for future cluster insecure local registry
  block:
    - name: Clean directory for certs
      file:
        path: "/etc/containerd/certs.d/"
        state: absent

    - name: Create directory for certs
      file:
        path: "/etc/containerd/certs.d/{{ registry_ip }}"
        state: directory

    - name: Add host to insecure registry
      template:
        src: insecure-registry.toml.j2
        dest: "/etc/containerd/certs.d/{{ registry_ip }}/hosts.toml"

    - name: Create directory for certs
      file:
        path: "/etc/containerd/certs.d/registry.shared-services"
        state: directory

    - name: Add host to insecure registry
      template:
        src: insecure-registry.toml.j2
        dest: "/etc/containerd/certs.d/registry.shared-services/hosts.toml"

    - name: Create directory for dockerhub
      file:
        path: "/etc/containerd/certs.d/registry-1.docker.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://registry-1.docker.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5000"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/registry-1.docker.io/hosts.toml

    - name: Create directory for k8s
      file:
        path: "/etc/containerd/certs.d/registry.k8s.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://registry.k8s.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5001"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/registry.k8s.io/hosts.toml

    - name: Create directory for quay
      file:
        path: "/etc/containerd/certs.d/quay.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://quay.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5002"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/quay.io/hosts.toml

    - name: Create directory for gcr
      file:
        path: "/etc/containerd/certs.d/gcr.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://gcr.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5003"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/gcr.io/hosts.toml

    - name: Create directory for ghcr
      file:
        path: "/etc/containerd/certs.d/ghcr.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://ghcr.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5004"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/ghcr.io/hosts.toml

    - name: Create directory for lscr
      file:
        path: "/etc/containerd/certs.d/lscr.io"
        state: directory

    - name: Registry proxy configuration
      copy:
        content: |
          server = "https://lscr.io"
          [host."http://{{ hostvars['controller-001'].ansible_default_ipv4.address }}:5005"]
            capabilities = ["pull", "resolve"]
            skip_verify = true
        dest: /etc/containerd/certs.d/lscr.io/hosts.toml

    - name: Restart containerd
      service:
        name: containerd
        state: restarted

#############################################
# Setup with kubeadm
#############################################

- name: If node is controller, check if cluster is established and create cluster otherwise
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Check if cluster is established
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadmn_init

    - name: Create cluster if not created
      shell: kubeadm init --pod-network-cidr={{ k8s_cidr }}
      when: kubeadmn_init.stat.exists == false

    - name: Create remote config path
      file:
        path: /root/.kube
        state: directory

    - name: Copy kubeconfig to remote for future scripts
      copy:
        src: /etc/kubernetes/admin.conf
        remote_src: true
        dest: /root/.kube/config
        mode: "0600"

    - name: Generate token just in case we need it for the next task
      shell: kubeadm token create --print-join-command
      register: kubeadm_token_create

- name: If node is worker, check if cluster is established and join cluster otherwise
  when: inventory_hostname in groups["workers"]
  block:
    - name: Check if cluster is established
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubeadm_join

    - name: Join cluster if not joined
      shell: "{{ hostvars[item].kubeadm_token_create.stdout_lines[0] }}"
      with_items: "{{ groups['controllers'] }}"
      when: kubeadm_join.stat.exists == false

- name: Clean up kubeadm token
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Get token list
      shell: kubeadm token list
      register: kubeadm_token_list

    - name: Delete token
      shell: kubeadm token delete {{ kubeadm_token_list.stdout_lines[1].split()[0] }}

#############################################
# Pass around credentials for access to cluster
#############################################

- name: Steal admin config from controller for a bit
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Get config from controller
      slurp:
        src: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: Create .kube if not exists
      become: false
      delegate_to: localhost
      file:
        path: ~/.kube
        state: directory

    - name: Clean kubeconfig
      become: false
      delegate_to: localhost
      file:
        path: ~/.kube/config.hscluster
        state: absent

    - name: Copy kubeconfig to local
      become: false
      delegate_to: localhost
      copy:
        content: "{{ kubeconfig.content | b64decode | string }}"
        dest: ~/.kube/config.hscluster

    - name: Create remote config path
      file:
        path: /root/.kube
        state: directory

    - name: Copy kubeconfig to remote for future scripts
      copy:
        content: "{{ kubeconfig.content | b64decode | string }}"
        dest: /root/.kube/config
        mode: "0600"

    - name: Replace IP
      become: false
      delegate_to: localhost
      replace:
        path: ~/.kube/config.hscluster
        regexp: "0.0.0.0"
        replace: "{{ ansible_default_ipv4.address }}"

    - name: Replace CA
      become: false
      delegate_to: localhost
      replace:
        path: ~/.kube/config.hscluster
        regexp: "certificate-authority-data.*"
        replace: "insecure-skip-tls-verify: true"

#############################################
# Setup with core infrastructure
#############################################

- name: Deploy flannel
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Create flannel
      shell: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml #https://github.com/flannel-io/flannel

    - name: Wait for flannel to be ready
      shell: kubectl get pods -n kube-flannel -l app=flannel -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}'
      register: flannel_ready
      until: flannel_ready.stdout == "True"
      retries: 10
      delay: 10

- name: Deploy metallb
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Deploy metallb with defaults
      shell: kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml

    - name: Metallb namespace
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        state: present
        name: metallb-system

    - name: Configure metallb pool
      ignore_errors: true
      retries: 10
      delay: 20
      register: ippool
      until: ippool.failed == false
      kubernetes.core.k8s:
        state: present
        template: metallb-config.yml.j2

- name: Deploy kubevirt
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Install kubevirt # https://github.com/kubevirt/containerized-data-importer/blob/main/README.md TODO: Break this into composable parts
      shell: |
        kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/{{ lookup('ansible.builtin.url', 'https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt') }}/kubevirt-operator.yaml
        kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/{{ lookup('ansible.builtin.url', 'https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt') }}/kubevirt-cr.yaml
        export VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
        kubectl apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml
        kubectl apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml

    - name: Wait for kubevirt to complete
      shell: kubectl -n kubevirt wait kv kubevirt --for condition=Available
      register: kubevirt_ready
      until: kubevirt_ready.stdout == "kubevirt.kubevirt.io/kubevirt condition met"
      retries: 10
      delay: 20

    - name: Create namespace for kubevirt images
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        state: present
        name: kubevirt-images

  # NOTE: This is necessary if VMs are going on nested hypervisors. If nodes are all baremetal, then remove this.
  #  - name: Patch to use nested virtualization
  #    kubernetes.core.k8s:
  #      state: patched
  #      kind: KubeVirt
  #      name: kubevirt
  #      namespace: kubevirt
  #      api_version: kubevirt.io/v1
  #      definition:
  #        spec:
  #          configuration:
  #            developerConfiguration:
  #              useEmulation: true

- name: Create storage provisioners
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Install storage drivers
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
        kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml

- name: Set node labels and prep storage provisioner
  when: inventory_hostname in groups["controllers"]
  loop: "{{ groups['workers'] }}"
  shell: |
    kubectl label nodes {{ item }} architecture={{ hostvars[item]['node_label'] }}
    kubectl label node {{ item }} openebs.io/engine=mayastor

- name: Update hugepages
  when: inventory_hostname in groups["workers"]
  block:
    - name: Update hugepages
      lineinfile:
        line: vm.nr_hugepages = 1048
        name: /etc/sysctl.conf
        state: present
    - name: Replace nf_hugepages
      shell: |
        echo 1048 | tee /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
        modprobe nvme_tcp

    - name: Restart kubelet
      service:
        name: kubelet
        state: restarted

- name: Deploy helm to controller
  when: inventory_hostname in groups["controllers"]
  shell: curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash

- name: Prepare post-setup storage
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Install Mayastor repo
      shell: helm repo add mayastor https://openebs.github.io/mayastor-extensions/

    - name: Update helm
      ignore_errors: true
      shell: helm repo update

    - name: Install mayastor
      ignore_errors: true
      shell: helm install mayastor mayastor/mayastor -n openebs --create-namespace --version 2.5.0

    - name: Patch mayastore DS
      shell: |
        kubectl patch daemonset \
          mayastor-io-engine \
          --namespace openebs \
          --type='json' \
          -p='[{"op": "replace", "path": "/spec/template/spec/containers/1/args", "value": [
            "-g$(MY_POD_IP)",
            "-N$(MY_NODE_NAME)",
            "-Rhttps://mayastor-agent-core:50051",
            "-y/var/local/mayastor/io-engine/config.yaml",
            "-l1,2",
            "-p=mayastor-etcd:2379",
            "--ptpl-dir=/var/local/mayastor/io-engine/ptpl/",
            "--api-versions=v1",
            "--tgt-crdt=30",
            "--events-url=nats://mayastor-nats:4222",
            "--env-context=--iova-mode=pa"
        ]}]'

    - name: Reduce mayastor resources
      shell: |
        kubectl patch daemonset \
          mayastor-io-engine \
          --namespace openebs \
          --type='json' \
          -p='[{"op": "replace", "path": "/spec/template/spec/containers/1/resources", "value": {
            "limits": {
              "cpu": "0.5",
              "memory": "1Gi"
            },
            "requests": {
              "cpu": "0.5",
              "memory": "512Mi"
            }
          }}]'

    - name: Replace mayastor io engine
      shell: |
        kubectl delete pods -n openebs -l app=io-engine

    - name: Create mayastor 1 rv storageclass
      kubernetes.core.k8s:
        state: present
        template: mayastor-1xstorageclass.yml.j2

    - name: Create mayastor 3 rv storageclass
      kubernetes.core.k8s:
        state: present
        template: mayastor-3xstorageclass.yml.j2

    - name: Create local-host storageclass
      kubernetes.core.k8s:
        state: present
        template: local-path.yml.j2

    - name: Patch local-host path provisioner to existing path
      shell: |
        kubectl patch configmap/local-path-config -n local-path-storage --type merge -p '{"data":{"config.json":"{\"nodePathMap\":[{\"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\"paths\":[\"/opt/local-path-provisioner\"]}]}"}}'

    - name: Set node labels and prep storage provisioner
      loop: "{{ groups['workers'] }}"
      retries: 10
      delay: 20
      register: diskpool
      until: diskpool.failed == false
      kubernetes.core.k8s:
        state: present
        template: mayastor-diskpool.yaml.j2

- name: Install observability
  when: inventory_hostname in groups["controllers"]
  block:
    - name: Install prometheus repo
      shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

    - name: Update helm
      ignore_errors: true
      shell: helm repo update

    - name: Install prometheus with persistent volume
      ignore_errors: true
      kubernetes.core.helm:
        name: kps
        chart_ref: "prometheus-community/kube-prometheus-stack"
        create_namespace: true
        release_namespace: monitoring
        state: present
        release_values:
          grafana:
            enabled: false
          prometheus:
            prometheusSpec:
              storageSpec:
                volumeClaimTemplate:
                  spec:
                    storageClassName: "mayastor-3-rv"
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: 50Gi

    - name: Install Metrics-API
      kubernetes.core.helm:
        name: metrics-server
        chart_ref: metrics-server
        chart_repo_url: https://kubernetes-sigs.github.io/metrics-server/
        create_namespace: true
        release_namespace: monitoring
        state: present
        release_values:
          args:
            - --kubelet-insecure-tls
